{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e621837-05b4-4424-add1-99c60d9e1754",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eigen_tech_project.nlp_models import nltk_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b8fb88-cda8-404d-90ac-7044701a242e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5e546b-f973-4388-b3b9-cd6cb3c83b73",
   "metadata": {},
   "source": [
    "## Load in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec6463fc-c57d-4b52-acee-140aafe771a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join, abspath\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "class DataReaderParser:\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.splitter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        \n",
    "    def __repr__(self):\n",
    "        '''Returns representation of the DataLoader object'''\n",
    "        return(\"{}({!r})\".format(self.__class__.__name__, self.path))\n",
    "    \n",
    "    @property\n",
    "    def file_names(self):\n",
    "        '''Returns representation of the DataLoader object'''\n",
    "        return [f for f in listdir(self.path) if isfile(join(self.path, f))]\n",
    "    \n",
    "    @property\n",
    "    def raw_data(self):\n",
    "        '''Returns representation of the DataLoader object'''\n",
    "        return [{\"file\": int(re.sub(\"[^0-9]\", \"\", f)), \n",
    "                 \"body\": open(abspath(join(self.path, f)), 'r').read()} for f in self.file_names]\n",
    "    \n",
    "    @property\n",
    "    def sentences(self):\n",
    "        '''Returns representation of the DataLoader object'''  \n",
    "        data = []\n",
    "        for file in self.raw_data:\n",
    "            sentences = self.splitter.tokenize(file['body'])\n",
    "            data.extend([{'sentence': sentence, 'file': file['file']} for sentence in sentences])\n",
    "            \n",
    "        return data\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e91f7e-112f-49cc-b899-1ddd2daa5c55",
   "metadata": {},
   "source": [
    "## Document->Sentences->Tokens->POS->Lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e622df5-e31f-4420-b8d1-a6fa4ab78b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "#example text text = 'What can I say about this place. The staff of these restaurants is nice and the eggplant is not bad'\n",
    "\n",
    "class LemmatizationWithPOSTagger:\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        '''Returns representation of the DataLoader object'''\n",
    "        return(\"{}({!r})\".format(self.__class__.__name__))\n",
    "    \n",
    "    def get_wordnet_pos(self, treebank_tag):\n",
    "        \"\"\"\n",
    "        return WORDNET POS compliance to WORDNET lemmatization (a,n,r,v) \n",
    "        \"\"\"\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            # As default pos in lemmatization is Noun\n",
    "            return wordnet.NOUN\n",
    "        \n",
    "    def get_lemma(self, word_postag_combo):\n",
    "        \"\"\"\n",
    "        return WORDNET POS compliance to WORDNET lemmatization (a,n,r,v) \n",
    "        \"\"\"\n",
    "        return self.lemmatizer.lemmatize(word_postag_combo[0], self.get_wordnet_pos(word_postag_combo[1]))\n",
    "\n",
    "    def lemmas(self, tokenized_sentence):\n",
    "        \"\"\"\n",
    "        return WORDNET POS compliance to WORDNET lemmatization (a,n,r,v) \n",
    "        \"\"\"\n",
    "        # find the pos tagging for each tokens [('What', 'WP'), ('can', 'MD'), ('I', 'PRP') ....\n",
    "        pos_tokens = nltk.pos_tag(tokenized_sentence)\n",
    "        # lemmatization using pos tags\n",
    "        # convert into feature set of [('What', 'What', ['WP']), ('can', 'can', ['MD']), ... ie [original WORD, Lemmatized word, POS tag]\n",
    "        lemmas = [self.get_lemma(word_tag_combo) for word_tag_combo in pos_tokens]\n",
    "        return lemmas\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e535a5ec-f593-4d30-b1c1-511822209fc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "\n",
    "class Sentence:\n",
    "    \"\"\"\n",
    "    split the document into sentences and tokenize each sentence\n",
    "    \"\"\"\n",
    "    def __init__(self, sentence):\n",
    "        self.sentence = sentence\n",
    "#         self.tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "#         self.tokenizer = nltk.tokenize.punkt.PunktSentenceTokenizer()\n",
    "        # following tokenizer also removes punctuation:\n",
    "        self.tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "        self.lemmatizer = LemmatizationWithPOSTagger()\n",
    "        self.stopwords = nltk.corpus.stopwords.words('english')\n",
    "        self.common_words_url = \"https://gist.githubusercontent.com/deekayen/4148741/raw/98d35708fa344717d8eee15d11987de6c8e26d7d/1-1000.txt\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        '''Returns representation of the DataLoader object'''\n",
    "        return (\"{}({!r})\".format(self.__class__.__name__, self.sentence))\n",
    "    \n",
    "    @property\n",
    "    def tokenized_sentence(self):\n",
    "        \"\"\"\n",
    "        Return a list of sublists with tokens. \n",
    "        \"\"\"\n",
    "        # tokenization in each sentences\n",
    "        return self.tokenizer.tokenize(self.sentence.lower())\n",
    "    \n",
    "    @property\n",
    "    def lemmatized_sentence(self):\n",
    "        \"\"\"\n",
    "        Return a list of sublists with lemmas. \n",
    "        \"\"\"\n",
    "        return self.lemmatizer.lemmas(self.tokenized_sentence)\n",
    "    \n",
    "    def remove_stopwords(self, text):\n",
    "        \"\"\"\n",
    "        return WORDNET POS compliance to WORDNET lemmatization (a,n,r,v) \n",
    "        \"\"\"\n",
    "#         common_words = requests.get(self.common_words_url).text.split()\n",
    "#         noise = self.stopwords + common_words\n",
    "        return [w for w in text if w not in self.stopwords and w.isalpha()]\n",
    "    \n",
    "    @property\n",
    "    def lemmatized_sentence_no_stop(self):\n",
    "        \"\"\"\n",
    "        Return a list of sublists with interesting lemmas. \n",
    "        Ergo: stopword and non-alphabetical removal. \n",
    "        \"\"\"\n",
    "        return self.remove_stopwords(self.lemmatized_sentence)\n",
    "    \n",
    "    @property\n",
    "    def clean_sentence(self):\n",
    "        \"\"\"\n",
    "        Return a single sentence.\n",
    "        \"\"\"\n",
    "        return \" \".join(self.lemmatized_sentence_no_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3449ebe0-1720-4ea7-bab7-8cf37e1535f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words_url = \"https://gist.githubusercontent.com/deekayen/4148741/raw/98d35708fa344717d8eee15d11987de6c8e26d7d/1-1000.txt\"\n",
    "common_words = requests.get(common_words_url).text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "86455b7f-90a1-4d9f-8197-520ae038fa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_sentence'] = df.sentence.apply(lambda x: Sentence(x).clean_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f549e6-a9a9-4bb2-9417-d9ca214e5c7d",
   "metadata": {},
   "source": [
    "## Construct the countvectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41848795-ae92-4af7-a3c0-788364601dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "class InvertedIndex:\n",
    "    def __init__(self, corpus):\n",
    "        self.corpus = corpus\n",
    "        self.countvectorizer = CountVectorizer()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        '''Returns representation of the DataLoader object'''\n",
    "        return(\"{}({!r})\".format(self.__class__.__name__, self.corpus))\n",
    "    \n",
    "    @property\n",
    "    def inverted_index(self):\n",
    "        \"\"\"Return list with the top x most occuring interesting words, with following elements: (feature_id, occurence).\"\"\"\n",
    "        \n",
    "        vectorizer = self.countvectorizer.fit(self.corpus)\n",
    "        document_term_matrix = vectorizer.transform(self.corpus)\n",
    "        \n",
    "        lemmas = vectorizer.get_feature_names()\n",
    "        indices = range(0,len(lemmas))\n",
    "        frequencies = np.asarray(document_term_matrix.toarray().sum(axis=0))\n",
    "        occurences = [document_term_matrix[:,i].nonzero()[0].tolist() for i in indices]\n",
    "        \n",
    "        return [{'lemma': lemmas[i], \n",
    "                 'frequency': frequencies[i], \n",
    "                 'occurences': occurences[i]} for i in indices]\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5fa38dfc-429f-4827-bc10-c249907ce1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = DataReaderParser('data/').sentences\n",
    "y = [dict(item, **{'clean': Sentence(item[\"sentence\"]).clean_sentence}) for item in x]\n",
    "\n",
    "df_input = pd.DataFrame(y).reset_index()\n",
    "inverted_index = InvertedIndex(df_input.clean).inverted_index\n",
    "df_ii = pd.DataFrame(inverted_index).sort_values('frequency', ascending=False)\n",
    "\n",
    "df_ii[\"sentences\"] = df_ii.occurences.apply(lambda x: df_input.iloc[x].sentence.tolist())\n",
    "df_ii[\"documents\"] = df_ii.occurences.apply(lambda x: list(set(df_input.iloc[x].file)))\n",
    "df_ii = df_ii.drop([\"occurences\"], axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c5b689fc-1848-4153-910d-7a29a817b5fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma</th>\n",
       "      <th>frequency</th>\n",
       "      <th>sentences</th>\n",
       "      <th>documents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>american</td>\n",
       "      <td>73</td>\n",
       "      <td>[But, when I think about what is at stake I am...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iraq</td>\n",
       "      <td>64</td>\n",
       "      <td>[A few Tuesdays ago, the American people embra...</td>\n",
       "      <td>[1, 2, 3, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>america</td>\n",
       "      <td>54</td>\n",
       "      <td>[A few Tuesdays ago, the American people embra...</td>\n",
       "      <td>[1, 2, 3, 4, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>government</td>\n",
       "      <td>47</td>\n",
       "      <td>[And the Kiev story is heading in the right di...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>promise</td>\n",
       "      <td>40</td>\n",
       "      <td>[One statistic powerfully describes this unful...</td>\n",
       "      <td>[1, 2, 3, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>threat</td>\n",
       "      <td>35</td>\n",
       "      <td>[Now, few people understand these challenges b...</td>\n",
       "      <td>[2, 3, 5, 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>iraqi</td>\n",
       "      <td>34</td>\n",
       "      <td>[We have been told that progress is just aroun...</td>\n",
       "      <td>[1, 2, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>weapon</td>\n",
       "      <td>31</td>\n",
       "      <td>[As some of you know, Senator Lugar and I rece...</td>\n",
       "      <td>[1, 2, 4, 5, 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>kenya</td>\n",
       "      <td>31</td>\n",
       "      <td>[The first time I came to Kenya was in 1987., ...</td>\n",
       "      <td>[2, 3, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>troop</td>\n",
       "      <td>30</td>\n",
       "      <td>[Besides the devastation they can cause to a c...</td>\n",
       "      <td>[1, 2, 3, 5, 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>today</td>\n",
       "      <td>29</td>\n",
       "      <td>[Today, experts tell us that we're in a race a...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>health</td>\n",
       "      <td>23</td>\n",
       "      <td>[Not to mention the environmental and public h...</td>\n",
       "      <td>[1, 2, 3, 4, 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>president</td>\n",
       "      <td>22</td>\n",
       "      <td>[But, when I think about what is at stake I am...</td>\n",
       "      <td>[1, 2, 3, 5, 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>program</td>\n",
       "      <td>21</td>\n",
       "      <td>[Now, few people understand these challenges b...</td>\n",
       "      <td>[1, 2, 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>security</td>\n",
       "      <td>21</td>\n",
       "      <td>[We entered through no fences or discernible s...</td>\n",
       "      <td>[2, 3, 5, 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>corruption</td>\n",
       "      <td>21</td>\n",
       "      <td>[One that serves its people and is free from c...</td>\n",
       "      <td>[3, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>mccain</td>\n",
       "      <td>21</td>\n",
       "      <td>[The Republican nominee, John McCain, has worn...</td>\n",
       "      <td>[2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>terrorist</td>\n",
       "      <td>20</td>\n",
       "      <td>[And as we speak, members of Al Qaeda and othe...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>generation</td>\n",
       "      <td>20</td>\n",
       "      <td>[My third recommendation - which I'll just tou...</td>\n",
       "      <td>[1, 2, 5, 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>kenyan</td>\n",
       "      <td>19</td>\n",
       "      <td>[Certainly it is not due to lack of effort on ...</td>\n",
       "      <td>[4]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         lemma  frequency                                          sentences  \\\n",
       "0     american         73  [But, when I think about what is at stake I am...   \n",
       "1         iraq         64  [A few Tuesdays ago, the American people embra...   \n",
       "2      america         54  [A few Tuesdays ago, the American people embra...   \n",
       "3   government         47  [And the Kiev story is heading in the right di...   \n",
       "4      promise         40  [One statistic powerfully describes this unful...   \n",
       "5       threat         35  [Now, few people understand these challenges b...   \n",
       "6        iraqi         34  [We have been told that progress is just aroun...   \n",
       "7       weapon         31  [As some of you know, Senator Lugar and I rece...   \n",
       "8        kenya         31  [The first time I came to Kenya was in 1987., ...   \n",
       "9        troop         30  [Besides the devastation they can cause to a c...   \n",
       "10       today         29  [Today, experts tell us that we're in a race a...   \n",
       "11      health         23  [Not to mention the environmental and public h...   \n",
       "12   president         22  [But, when I think about what is at stake I am...   \n",
       "13     program         21  [Now, few people understand these challenges b...   \n",
       "14    security         21  [We entered through no fences or discernible s...   \n",
       "15  corruption         21  [One that serves its people and is free from c...   \n",
       "16      mccain         21  [The Republican nominee, John McCain, has worn...   \n",
       "17   terrorist         20  [And as we speak, members of Al Qaeda and othe...   \n",
       "18  generation         20  [My third recommendation - which I'll just tou...   \n",
       "19      kenyan         19  [Certainly it is not due to lack of effort on ...   \n",
       "\n",
       "             documents  \n",
       "0   [1, 2, 3, 4, 5, 6]  \n",
       "1         [1, 2, 3, 5]  \n",
       "2      [1, 2, 3, 4, 5]  \n",
       "3   [1, 2, 3, 4, 5, 6]  \n",
       "4         [1, 2, 3, 4]  \n",
       "5         [2, 3, 5, 6]  \n",
       "6            [1, 2, 5]  \n",
       "7      [1, 2, 4, 5, 6]  \n",
       "8            [2, 3, 4]  \n",
       "9      [1, 2, 3, 5, 6]  \n",
       "10  [1, 2, 3, 4, 5, 6]  \n",
       "11     [1, 2, 3, 4, 6]  \n",
       "12     [1, 2, 3, 5, 6]  \n",
       "13           [1, 2, 6]  \n",
       "14        [2, 3, 5, 6]  \n",
       "15              [3, 4]  \n",
       "16                 [2]  \n",
       "17  [1, 2, 3, 4, 5, 6]  \n",
       "18        [1, 2, 5, 6]  \n",
       "19                 [4]  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ii[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "5a06567b-d102-4983-b43f-f3b1d3a86dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = CountVectorizer()\n",
    "\n",
    "\n",
    "\n",
    "y = x.fit(corpus)\n",
    "\n",
    "z = y.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "1d816027-d0a6-4a8f-8079-897f24ca4bc0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "get_feature_names not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-448-5ec5ba5c05f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/share/virtualenvs/eigen_tech_project-7cRe8WY7/lib/python3.9/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" not found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: get_feature_names not found"
     ]
    }
   ],
   "source": [
    "z.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2346d0-5235-4708-b682-d67554706249",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "022b6ff2-675c-46c2-9215-62e9898b21f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "x = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0958253e-0134-4220-8c6f-8d6abfa87c1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a16bc43-13f5-4e7c-a687-5bc0e6765172",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "3ed1c94c-1cb9-4cf5-836f-41ec0a4e18bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = DataLoader(\"data/\")\n",
    "\n",
    "df = x.df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0342bf6-7f45-4647-8859-2f5717c1f716",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "1eeec066-03f0-42e1-ba0b-f9caaaddbae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = TextParser(text=df.body[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "52b25888-7e29-44a3-ace5-237c20573391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y.tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "5e35108e-0d9b-4834-b335-92b324bedbbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y.lemmatized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e426f717-2632-4b0c-965d-a8f826788de9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a7b1f4-cb89-4ddb-8984-8f3ab999cff6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879d092d-0459-4def-b565-dea3aff63c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715f33bd-ecfe-48b3-ae5f-446662677b03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbe503e-5ab6-41e7-83eb-7afa024f64fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44380f0d-ee0f-4fe7-b8d6-35c2c1202d48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687c90c8-4df8-475d-b412-3ab516d2a293",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
